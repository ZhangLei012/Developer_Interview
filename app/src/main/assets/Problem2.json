[
  {
    "answer": "强化学习（Reinforcement Learning, RL），又称增强学习，是机器学习的范式和方法论之一，用于描述和解决智能体（agent）在与环境的交互过程中通过学习策略以达成回报最大化或实现特定目标的问题 </br></br>",
    "content": "",
    "id": "0003",
    "source": "https://github.com/TotemsCN/Base/blob/master/Java%20SE/Java.md",
    "title": "什么是强化学习？",
    "type": 1
  },
  {
    "answer": "解答：</br></br>监督学习一般有标签信息，而且是单步决策问题，比如分类问题。监督学习的样本一般是独立 同分布的。无监督学习没有任何标签信息，一般对应的是聚类问题。强化学习介于监督和无监督学 习之间，每一步决策之后会有一个标量的反馈信号，即回报。通过最大化回报以获得一个最优策略。 因此强化学习一般是多步决策，并且样本之间有强的相关性。",
    "content": "无",
    "id": "0004",
    "source": "https://github.com/TotemsCN/Base/blob/master/Java%20SE/Java.md",
    "title": "强化学习和监督学习、无监督学习的区别是什么？",
    "type": 1
  },
  {
    "answer": "解答：</br></br>强化学习适合于解决模型未知，且当前决策会影响环境状态的（序列）决策问题。Bandit问题可以 看成是一种特殊的强化学习问题，序列长度为1，单步决策后就完事了,所以动作不影响状态。当然也有影响 的bandit问题，叫做contextual bandit问题。",
    "content": "无",
    "id": "0005",
    "source": "https://github.com/TotemsCN/Base/blob/master/Java%20SE/Java.md",
    "title": "强化学习适合解决什么样子的问题？",
    "type": 1
  },
  {
    "answer": "解答：</br></br>累积回报。依赖于不同的问题设定，累积回报具有不同的形式。比如对于有限长度的MDP问题直接用 回报和作为优化目标。对于无限长的问题，为了保证求和是有意义的，需要使用折扣累积回报或者平均 回报。深度学习的损失函数一般是多个独立同分布样本预测值和标签值的误差，需要最小化。强化学习 的损失函数是轨迹上的累积和，需要最大化。</br>",
    "content": "无",
    "id": "0007",
    "source": "https://github.com/TotemsCN/Base/blob/master/Java%20SE/Java.md",
    "title": "强化学习的损失函数（loss function）是什么？和深度学习的损失函数有何关系？",
    "type": 1
  },
  {
    "answer": "解答：</br></br>POMDP是部分可观测马尔科夫决策问题。 马尔科夫过程表示一个状态序列，每一个状态是一个随机变量，变量之间满足马尔科夫性，表示为 一个元组<S, P>，S是状态，P表示转移概率。 MDP表示为一个五元组<S, A, P, R, γ>，S是状态集合，A是动作集合，P表示转移概率，即模型， R是回报函数，γ表示折扣因子。 马尔科夫体现了无后效性，也就是说未来的决策之和当前的状态有关，和历史状态无关。",
    "content": "无",
    "id": "0008",
    "source": "https://github.com/TotemsCN/Base/blob/master/Java%20SE/Java.md",
    "title": "POMDP是什么？马尔科夫过程是什么？马尔科夫决策过程是什么？里面的“马尔科夫”体现了什么性质？",
    "type": 1
  },
  {
    "answer": "解析：</br></br>如果不满足马尔科夫性，强行只用当前的状态来决策，势必导致决策的片面性，得到不好的策略。 为了解决这个问题，可以利用RNN对历史信息建模，获得包含历史信息的状态表征。表征过程可以 使用注意力机制等手段。最后在表征状态空间求解MDP问题。",
    "content": "无",
    "id": "0009",
    "source": "https://github.com/TotemsCN/Base/blob/master/Java%20SE/Java.md",
    "title": "如果不满足马尔科夫性怎么办？当前时刻的状态和它之前很多很多个状态都有关之间关系？",
    "type": 1
  },
  {
    "answer": "解析：</br></br>方法有：动态规划，时间差分，蒙特卡洛。 有模型可以使用动态规划方法。 动态规划是指：将一个问题拆分成几个子问题，分别求解这些子问题，然后获得原问题的解。 贝尔曼方程中为了求解一个状态的值函数，利用了其他状态的值函数，就是这种思想（个人觉得）。",
    "content": "无",
    "id": "00010",
    "source": "https://github.com/TotemsCN/Base/blob/master/Java%20SE/Java.md",
    "title": "求解马尔科夫决策过程都有哪些方法？有模型用什么方法？动态规划是怎么回事？",
    "type": 1
  }
]